{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Semantic Segmentation using FCN\n",
    "이번 실습에서는 **VGG-16** network 구조에 **FCN**을 적용하여 _Semantic Segmentation_을 구현해보도록 한다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "\n",
    "import os\n",
    "import sys\n",
    "import math\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from PIL import Image\n",
    "from matplotlib import pyplot as plt\n",
    "import skimage.transform\n",
    "from skimage import io\n",
    "from numpy import ogrid, repeat, newaxis\n",
    "\n",
    "sys.path.append(os.path.join(os.getcwd(), '..'))\n",
    "from voc_loader import VOC_loader\n",
    "import fcn_vgg\n",
    "\n",
    "from loss import loss as get_loss\n",
    "from utils import *\n",
    "\n",
    "slim = tf.contrib.slim"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Segmentation 결과 비교\n",
    "아래 함수는 _original image_, _ground truth segmentation_, _predicted segmentation_을 순서대로 화면에 출력하는 코드이다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def plot_result(img, gt_seg, pred_seg, palette):\n",
    "    if pred_seg is None:\n",
    "        f, (ax1, ax2) = plt.subplots(1, 2)\n",
    "    else:\n",
    "        f, (ax1, ax2, ax3) = plt.subplots(1, 3)\n",
    "        \n",
    "    ax1.imshow(img.astype(np.uint8))\n",
    "    ax2.imshow(gt_seg.astype(np.uint8))\n",
    "    \n",
    "    if pred_seg is not None:\n",
    "        #np.place(pred_seg, pred_seg == 21, 255)\n",
    "        tmp = Image.fromarray(pred_seg.astype(np.uint8), 'P')\n",
    "        tmp.putpalette(palette)\n",
    "        ax3.imshow(tmp)\n",
    "    \n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Download PASCAL VOC 2012 dataset\n",
    "**PASCAL VOC** dataset은 classification, detection, segmentation 3개의 성능 평가를 지원하는 dataset으로 이번 실습에서는 **VOC 2012** segmentation dataset을 사용한다. dataset 구성은 'aeroplane', 'bicycle', 'bird', 'boat', 'bottle', 'bus', 'car', 'cat', 'chair', 'cow', 'diningtable', 'dog', 'horse', 'motorbike', 'person', 'potted plant', 'sheep', 'sofa', 'train', 'tv/monitor' 총 20개의 클래스로 이루어져 있다.\n",
    "\n",
    "자세한 dataset 구성은 아래 documentation에 나와있다.\n",
    "http://host.robots.ox.ac.uk/pascal/VOC/voc2012/htmldoc/index.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "if not os.path.exists('VOCdevkit'):\n",
    "    os.system('wget http://host.robots.ox.ac.uk/pascal/VOC/voc2012/VOCtrainval_11-May-2012.tar')\n",
    "    os.system('tar xvf VOCtrainval_11-May-2012.tar')\n",
    "    os.system('rm VOCtrainval_11-May-2012.tar')\n",
    "\n",
    "data_dir = 'vgg_models'\n",
    "data_url = 'http://download.tensorflow.org/models/vgg_16_2016_08_28.tar.gz'\n",
    "maybe_download_and_extract(data_url, data_dir, 'vgg_16.ckpt')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "### Load VOC 2012 Dataset\n",
    "다운 받은 **PASCAL VOC 2012** dataset을 읽어온다. **VOCdevkit/VOC2012** 폴더를 살펴보면 Segmentation에 사용될 image name이 저장되어 있는 txt 파일이 **split_root**에 있고 원본 image는 **image_root**에 있으며 **segmap_root**는 각 이미지 별로 grount truth segmentation에 대한 정보를 가지고 있다. 추후 **segmap_root**의 정보를 가지고 segmentation이 얼마나 잘 되었는 지를 평가할 것이다.\n",
    "\n",
    "FCN을 VGG network를 수정하여 구현할 것이므로 input image size는 224가 된다. _num_classes_는 **PASCAL VOC 2012** dataset의 20개의 클래스와 배경 픽셀, 그리고 image의 각 object의 경계선을 나타내는 (학습에 사용되지 않는 영역) void 픽셀을 포함하여 총 22개로 정의된다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "loader_params = {\n",
    "    'num_classes': 22,\n",
    "    'image_size': 224,\n",
    "    'split_root': 'VOCdevkit/VOC2012/ImageSets/Segmentation',\n",
    "    'image_root': 'VOCdevkit/VOC2012/JPEGImages',\n",
    "    'segmap_root': 'VOCdevkit/VOC2012/SegmentationClass',\n",
    "}\n",
    "loader = VOC_loader(loader_params)\n",
    "\n",
    "# get information for VOC\n",
    "class_names = loader.get_class_names()\n",
    "num_classes = loader_params['num_classes']\n",
    "img_size = loader_params['image_size']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Check the dataset\n",
    "dataset이 잘 load 되었는 지 확인하기 위해 5개의 images를 출력해보는 부분이다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "check_batchs = 5\n",
    "batch = loader.get_batch(check_batchs)\n",
    "\n",
    "for ii in range (check_batchs):\n",
    "    plot_result(batch['images'][ii], batch['seg_maps'][ii], \n",
    "                None, None)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Parameters\n",
    "이전 실습과 마찬가지로 학습에 사용될 parameters를 정의한 부분이다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "initial_learning_rate = 0.0001 # initial learning rate for optimizer\n",
    "\n",
    "batch_size = 4\n",
    "num_epochs = 20\n",
    "\n",
    "iteration_per_epoch = int(math.floor(loader.get_num_train_examples() / batch_size))\n",
    "save_checkpoint_frequency = int(math.floor(iteration_per_epoch / 5))\n",
    "print_frequency = 50\n",
    "\n",
    "print(\"number of training data: %d\" % loader.get_num_train_examples())\n",
    "print(\"batch size: %d\" % batch_size)\n",
    "print(\"iterations per epoch: %d\" % iteration_per_epoch)\n",
    "print(\"number of epoch: %d\" % num_epochs)\n",
    "print(\"save_frequency: %d\" % save_checkpoint_frequency)\n",
    "\n",
    "net_type = 'fcn_32s'\n",
    "save_dir = '%s_checkpoints' % net_type\n",
    "save_type = ( save_dir + '/%s_vgg') % net_type\n",
    "save_path = ( save_type + '-%d') % num_epochs\n",
    "\n",
    "print(save_path)\n",
    "\n",
    "if not os.path.exists(save_dir): os.makedirs(save_dir)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create vgg-16 FCN Model\n",
    "**VGG-16** network를 fcn_vgg.py (Slim library를 이용해 network를 구성한 module)을 통해 생성한다. net_type에 따라 network 끝 단에 FCN이 구현된 방식이 달라진다. (fcn_vgg.py 파일 참조)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#with tf.Session() as sess:\n",
    "tf.reset_default_graph()\n",
    "\n",
    "with tf.variable_scope('inputs') as scope:\n",
    "    images = tf.placeholder(dtype=tf.float32, \n",
    "                            shape=[batch_size, img_size, img_size, 3],\n",
    "                            name='images')\n",
    "    labels = tf.placeholder(dtype=tf.int64, \n",
    "                            shape=[batch_size, img_size, img_size], \n",
    "                            name='labels')\n",
    "\n",
    "# build FCN\n",
    "vgg_fcn = fcn_vgg.FCN()\n",
    "vgg_fcn.build(images, net_type=net_type, train=True, num_classes=num_classes, debug=False)\n",
    "\n",
    "# Set the variables to be restored\n",
    "include_layers = ['vgg_16/conv1', 'vgg_16/conv2', 'vgg_16/conv3', 'vgg_16/conv4',\n",
    "                 'vgg_16/conv5', 'vgg_16/fc6', 'vgg_16/fc7']\n",
    "\n",
    "variables_to_restore = slim.get_variables_to_restore(include=include_layers)\n",
    "variables_to_learn = slim.get_variables_to_restore()\n",
    "\n",
    "print('===> The list of variables to be restored:')\n",
    "for i in variables_to_restore: print(i.op.name)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "학습에 사용할 loss function과 optimizer를 정한다. 해당 실습에서는 cross_entropy와 softmax를 loss function으로 사용하였고, Gradient Descent알고리즘을 optimizer로 사용해 학습시켰다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# optimizer\n",
    "global_step = tf.Variable(initial_value=0, name='global_step', trainable=False)\n",
    "\n",
    "total_loss = get_loss(vgg_fcn.upscore, labels, num_classes)\n",
    "\n",
    "optimizer = tf.train.AdamOptimizer(learning_rate=initial_learning_rate)\n",
    "train_op = optimizer.minimize(total_loss, var_list=variables_to_learn, global_step=global_step)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train FCN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Open the session\n",
    "sess = tf.Session()\n",
    "sess.run(tf.global_variables_initializer())\n",
    "\n",
    "# Create saver and restorer\n",
    "saver = tf.train.Saver(max_to_keep=5)\n",
    "restorer = tf.train.Saver(variables_to_restore)\n",
    "\n",
    "# select your training checkpoint for new-run\n",
    "checkpoint_path = ''\n",
    "\n",
    "if checkpoint_path != '':\n",
    "    saver.restore(sess, save_path=checkpoint_path)\n",
    "    print('Model is restored from %s' % checkpoint_path)\n",
    "else:\n",
    "    restorer.restore(sess, save_path='vgg_models/vgg_16.ckpt')\n",
    "    print('Model is restored from vgg_16 model')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "loader.reset()\n",
    "\n",
    "for ie in range(num_epochs):\n",
    "    for ii in range(iteration_per_epoch):\n",
    "        # Load a batch data\n",
    "        batch = loader.get_batch(batch_size, 'train')\n",
    "        feed_dict = {images: batch['images'], labels: batch['seg_labels']}\n",
    "\n",
    "        # Run the optimizer\n",
    "        learning = 'all'\n",
    "        tensors = [total_loss, train_op]\n",
    "        tf_loss, _ = sess.run(tensors, feed_dict=feed_dict)\n",
    "        # Print the accuracy and loss of current batch data\n",
    "        if (ii+1) % print_frequency == 0:\n",
    "            print('%d Epoch %d iteration - Loss (%.3f) - Learning - %s' % (ie+1, ii+1, tf_loss, learning))\n",
    "\n",
    "        # Save checkpoint\n",
    "        if (ii+1) % save_checkpoint_frequency == 0:\n",
    "            saver.save(sess, save_path=save_path, global_step=ie*iteration_per_epoch + ii + 1)\n",
    "            print('Saved checkpoint %s_%d' % (save_path, ie*iteration_per_epoch + ii + 1))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Visualize the result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# load test data and \n",
    "#loader.reset()\n",
    "\n",
    "batch = loader.get_batch(batch_size, 'test')\n",
    "\n",
    "tensors = [vgg_fcn.pred_up]\n",
    "feed_dict = {images: batch['images'], labels: batch['seg_labels']}\n",
    "\n",
    "# forward network to obtain segmentation result\n",
    "score  = sess.run(tensors, feed_dict=feed_dict)\n",
    "\n",
    "for bi in range(batch_size):\n",
    "    plot_result(batch['images'][bi], batch['seg_maps'][bi], score[0][bi], loader.get_palette())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## TEST the model\n",
    "### Load the checkpoint"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# reset the graph and session\n",
    "tf.reset_default_graph()\n",
    "sess.close()\n",
    "\n",
    "with tf.variable_scope('inputs') as scope:\n",
    "    images = tf.placeholder(dtype=tf.float32, \n",
    "                            shape=[batch_size, img_size, img_size, 3],\n",
    "                            name='images')\n",
    "    labels = tf.placeholder(dtype=tf.int64, \n",
    "                            shape=[batch_size, img_size, img_size], \n",
    "                            name='labels')\n",
    "\n",
    "# build FCN\n",
    "vgg_fcn = fcn_vgg.FCN()\n",
    "with tf.name_scope(\"content_vgg\"):\n",
    "    vgg_fcn.build(images, net_type=net_type, train=False, num_classes=num_classes, debug=False)\n",
    "    \n",
    "# optimizer\n",
    "global_step = tf.Variable(initial_value=0, name='global_step', trainable=False)\n",
    "\n",
    "total_loss = get_loss(vgg_fcn.upscore, labels, num_classes)\n",
    "train_op = tf.train.AdamOptimizer(initial_learning_rate).minimize(total_loss, global_step=global_step)\n",
    "\n",
    "sess = tf.Session()\n",
    "init = tf.global_variables_initializer()\n",
    "sess.run(init)\n",
    "\n",
    "# Automatically find the last checkpoint\n",
    "checkpoint_path = tf.train.latest_checkpoint(checkpoint_dir=save_dir)\n",
    "print('Last checkpoint path is %s' % (checkpoint_path))\n",
    "\n",
    "# Create saver\n",
    "saver = tf.train.Saver()\n",
    "if checkpoint_path != '':\n",
    "    saver.restore(sess, save_path=checkpoint_path)\n",
    "    # You can use specific checkpoint_path\n",
    "    '''\n",
    "    saver.restore(sess, save_path='')\n",
    "    '''\n",
    "    print('Model is restored from %s' % checkpoint_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Compute the accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Predict segmentation labels for all test examples\n",
    "# and compute confusion matrix\n",
    "num_classes = loader_params['num_classes']\n",
    "num_iterations = loader.get_num_test_examples() / batch_size\n",
    "conf_counts = np.zeros((num_classes, num_classes))\n",
    "\n",
    "loader.reset()\n",
    "for ii in range(int(num_iterations)):\n",
    "    # Load a batch data\n",
    "    batch = loader.get_batch(batch_size, 'test')\n",
    "    feed_dict = {images: batch['images'], labels: batch['seg_labels']}\n",
    "\n",
    "    # Run the optimizer\n",
    "    score = sess.run(vgg_fcn.pred_up, feed_dict=feed_dict)\n",
    "\n",
    "    # Accumulate confusions\n",
    "    for bi in range(batch_size):\n",
    "        # Do not count boundary labels\n",
    "        loc = np.where(batch['seg_labels'][bi] < 21, True, False)\n",
    "        # row is gt labels and column is predicted labels\n",
    "        sumim = batch['seg_labels'][bi] + score[bi] * num_classes\n",
    "        hs, bin_edge = np.histogram(sumim[loc], np.arange(num_classes*num_classes+1), \n",
    "                                    (0, num_classes*num_classes+1))\n",
    "        conf_counts = conf_counts + np.reshape(hs, (num_classes,num_classes))\n",
    "    \n",
    "    # Print the accuracy and loss of current batch data\n",
    "    if ((ii+1) % print_frequency == 0) or ((ii+1) == num_iterations):\n",
    "        print('TEST %d/%d Done' % (ii+1, num_iterations))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compute accuracy for all classes and mean accuracy \n",
    "acc = np.zeros(num_classes)\n",
    "for ic in range(num_classes):\n",
    "    gt1 = np.sum(conf_counts[ic,:])\n",
    "    res1 = np.sum(conf_counts[:,ic])\n",
    "    gtlres = conf_counts[ic,ic]\n",
    "    acc[ic] = 100.0 * gtlres / (gt1 + res1 - gtlres)\n",
    "    if (ic > 0) and (ic < num_classes-1):\n",
    "        print('%s IoU %.3f' % (class_names[ic-1], acc[ic]))\n",
    "print('=============================================')\n",
    "print('Mean IoU %.3f' % (np.mean(acc[1:num_classes])))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
