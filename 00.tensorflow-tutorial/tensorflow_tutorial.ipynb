{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "Tensorflow Tutorial\n",
    "==================="
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Graph Construction and Session\n",
    "- Tensorflow는 기본적으로 값과 연산을 가지는 node들로 구성된 Directed Acyclic Graph를 형성하여 구현되는 구조를 가진다. \n",
    "\n",
    "### 1.1. tf.constant() & tf.Session()\n",
    "- tf.constant()는 변하지 않은 값을 가지는 node를 생성한다.\n",
    "- tf.constant(value, dtype=None, shape=None, name=\"Const', verify_shape=False)\n",
    "- tf.sesstion()은 형성된 graph를 GPU/CPU에 올려 실행(계산)하는 역할을 한다.\n",
    "\n",
    "#### 1.1.1. compute '3+4'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import tensorflow as tf"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- 3과 4를 담는 node를 각각 생성"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(<tf.Tensor 'Const:0' shape=() dtype=float32>, <tf.Tensor 'Const_1:0' shape=() dtype=float32>)\n",
      "3.0\n",
      "3.0\n"
     ]
    }
   ],
   "source": [
    "node1 = tf.constant(3.0, tf.float32)\n",
    "node2 = tf.constant(4.0) # also tf.float32 implicitly\n",
    "print(node1, node2)\n",
    "\n",
    "sess = tf.Session()\n",
    "print(sess.run('Const:0'))\n",
    "print(sess.run(node1))\n",
    "sess.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- tf.Session 을 통해서 graph의 node를 run하면 node 값을 계산하고 결과를 return된다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[3.0, 4.0]\n"
     ]
    }
   ],
   "source": [
    "sess = tf.Session()\n",
    "print(sess.run([node1, node2]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "node3:  Tensor(\"Add:0\", shape=(), dtype=float32)\n",
      "sess.run(node3):  7.0\n"
     ]
    }
   ],
   "source": [
    "node3 = tf.add(node1, node2)\n",
    "print \"node3: \",node3\n",
    "print \"sess.run(node3): \",sess.run(node3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "#### 1.1.2. Compute ‘((3+4)^2)*5’"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3 + 4 =  7.0\n",
      "(3 + 4)^2 =  49.0\n",
      "((3 + 4)^2) * 5 =  245.0\n"
     ]
    }
   ],
   "source": [
    "node1 = tf.constant(3.0, tf.float32)\n",
    "node2 = tf.constant(4.0, tf.float32)\n",
    "node3 = tf.add(node1, node2)\n",
    "node4 = tf.constant(2.0, tf.float32)\n",
    "node5 = tf.pow(node3, node4)\n",
    "node6 = tf.constant(5.0, tf.float32)\n",
    "node7 = tf.multiply(node5, node6)\n",
    "\n",
    "print \"3 + 4 = \",sess.run(node3)\n",
    "print \"(3 + 4)^2 = \",sess.run(node5)\n",
    "print \"((3 + 4)^2) * 5 = \",sess.run(node7)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "### 1.2. tf.placeholder()\n",
    "- tf.placeholder는 graph를 run할 때 값을 입력해주어야 하는 node이다.\n",
    "- 함수의 input 즉 f(x)에서 x의 역할과도 같다.\n",
    "\n",
    "#### 1.2.1. Compute 'x+y'\n",
    "- tf.placeholder를 이용하여 sess.run할때마다 x,y에 input값을 주어 계산한다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "7.5\n",
      "[ 3.  7.]\n"
     ]
    }
   ],
   "source": [
    "x = tf.placeholder(tf.float32)\n",
    "y = tf.placeholder(tf.float32)\n",
    "adder = x + y\n",
    "\n",
    "z1 = sess.run(adder, {x: 3, y: 4.5})\n",
    "print(z1)\n",
    "\n",
    "z2 = sess.run(adder, {x: [1,3], y: [2, 4]})\n",
    "print(z2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 1.2.2. Compute '||Ax-y||'\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "A*x = \n",
      "[[  5.]\n",
      " [ 11.]]\n",
      "|Ax-y| = \n",
      "5.0\n"
     ]
    }
   ],
   "source": [
    "x = tf.placeholder(tf.float32)\n",
    "y = tf.placeholder(tf.float32)\n",
    "A = tf.constant([[1., 2.], [3., 4.]])\n",
    "y_ = tf.matmul(A,x)\n",
    "diff = y - y_\n",
    "err = tf.norm(diff)\n",
    "\n",
    "# should pass x to 2D matrix shape\n",
    "print \"A*x = \\n\",sess.run(y_, {x: [[1.], [2.]]})\n",
    "print \"|Ax-y| = \\n\",sess.run(err, {x: [[1.], [2.]], y:[[8.], [7.]]})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Variables\n",
    "- tf.Variable()는 tf.constant()와 같이 값이 고정된 것이 아니고 값을 변경할 수 있는 node이다.\n",
    "- 예를들어, f(x) = 3*x^2 + 2*x + 1 라 했을 때 x는 placeholder이고 3,2,1은 값을 변경할 상수라 하면 tf.constant이다.\n",
    "- 그런데, f(x) = a*x^2 + b*x + c 에서 필요에 따라 a,b,c 값을 변경해줄 수 있다면, 이들은 tf.Variable로 정의한다.\n",
    "- tf.Variable은 결국 function(graph)의 parameter이고, learning이 가능하다.\n",
    "- tf.Variable(initial_value, name=optional_name)\n",
    "\n",
    "### 2.1. Learning a Linear Model\n",
    "- 이 chapter에서는 주어진 데이터로부터 함수 f(x)를 선형회귀하는 모델 Wx+b 를 learning한다. \n",
    "- Initializer는 텐서플로우 variable들의 정의된 초기값을 부여해준다. \n",
    "- tf.global_initializer는 graph 내의 정의된 모든 variable을 초기화시키는 연산자.\n",
    "- 초기화 연산자를 sess.run()으로 실행시켜 초기화해준다. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "W = tf.Variable([.3], tf.float32)  # Variable adds learnable parameters in graph\n",
    "b = tf.Variable([-.3], tf.float32)\n",
    "x = tf.placeholder(tf.float32)\n",
    "linear_model = W * x + b"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- sess.run에 계산할 node 이름과 거기에 필요한 placeholder 값을 주어 결과를 뱉는다.\n",
    "- 하지만, variable들을 initialize하지 않고 graph를 run하면 에러 발생"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "ename": "FailedPreconditionError",
     "evalue": "Attempting to use uninitialized value Variable\n\t [[Node: Variable/read = Identity[T=DT_FLOAT, _class=[\"loc:@Variable\"], _device=\"/job:localhost/replica:0/task:0/device:GPU:0\"](Variable)]]\n\t [[Node: add_1/_21 = _Recv[client_terminated=false, recv_device=\"/job:localhost/replica:0/task:0/device:CPU:0\", send_device=\"/job:localhost/replica:0/task:0/device:GPU:0\", send_device_incarnation=1, tensor_name=\"edge_12_add_1\", tensor_type=DT_FLOAT, _device=\"/job:localhost/replica:0/task:0/device:CPU:0\"]()]]\n\nCaused by op u'Variable/read', defined at:\n  File \"/home/hanul/anaconda2/lib/python2.7/runpy.py\", line 162, in _run_module_as_main\n    \"__main__\", fname, loader, pkg_name)\n  File \"/home/hanul/anaconda2/lib/python2.7/runpy.py\", line 72, in _run_code\n    exec code in run_globals\n  File \"/home/hanul/anaconda2/lib/python2.7/site-packages/ipykernel_launcher.py\", line 16, in <module>\n    app.launch_new_instance()\n  File \"/home/hanul/anaconda2/lib/python2.7/site-packages/traitlets/config/application.py\", line 658, in launch_instance\n    app.start()\n  File \"/home/hanul/anaconda2/lib/python2.7/site-packages/ipykernel/kernelapp.py\", line 477, in start\n    ioloop.IOLoop.instance().start()\n  File \"/home/hanul/anaconda2/lib/python2.7/site-packages/zmq/eventloop/ioloop.py\", line 177, in start\n    super(ZMQIOLoop, self).start()\n  File \"/home/hanul/anaconda2/lib/python2.7/site-packages/tornado/ioloop.py\", line 888, in start\n    handler_func(fd_obj, events)\n  File \"/home/hanul/anaconda2/lib/python2.7/site-packages/tornado/stack_context.py\", line 277, in null_wrapper\n    return fn(*args, **kwargs)\n  File \"/home/hanul/anaconda2/lib/python2.7/site-packages/zmq/eventloop/zmqstream.py\", line 440, in _handle_events\n    self._handle_recv()\n  File \"/home/hanul/anaconda2/lib/python2.7/site-packages/zmq/eventloop/zmqstream.py\", line 472, in _handle_recv\n    self._run_callback(callback, msg)\n  File \"/home/hanul/anaconda2/lib/python2.7/site-packages/zmq/eventloop/zmqstream.py\", line 414, in _run_callback\n    callback(*args, **kwargs)\n  File \"/home/hanul/anaconda2/lib/python2.7/site-packages/tornado/stack_context.py\", line 277, in null_wrapper\n    return fn(*args, **kwargs)\n  File \"/home/hanul/anaconda2/lib/python2.7/site-packages/ipykernel/kernelbase.py\", line 283, in dispatcher\n    return self.dispatch_shell(stream, msg)\n  File \"/home/hanul/anaconda2/lib/python2.7/site-packages/ipykernel/kernelbase.py\", line 235, in dispatch_shell\n    handler(stream, idents, msg)\n  File \"/home/hanul/anaconda2/lib/python2.7/site-packages/ipykernel/kernelbase.py\", line 399, in execute_request\n    user_expressions, allow_stdin)\n  File \"/home/hanul/anaconda2/lib/python2.7/site-packages/ipykernel/ipkernel.py\", line 196, in do_execute\n    res = shell.run_cell(code, store_history=store_history, silent=silent)\n  File \"/home/hanul/anaconda2/lib/python2.7/site-packages/ipykernel/zmqshell.py\", line 533, in run_cell\n    return super(ZMQInteractiveShell, self).run_cell(*args, **kwargs)\n  File \"/home/hanul/anaconda2/lib/python2.7/site-packages/IPython/core/interactiveshell.py\", line 2718, in run_cell\n    interactivity=interactivity, compiler=compiler, result=result)\n  File \"/home/hanul/anaconda2/lib/python2.7/site-packages/IPython/core/interactiveshell.py\", line 2822, in run_ast_nodes\n    if self.run_code(code, result):\n  File \"/home/hanul/anaconda2/lib/python2.7/site-packages/IPython/core/interactiveshell.py\", line 2882, in run_code\n    exec(code_obj, self.user_global_ns, self.user_ns)\n  File \"<ipython-input-8-5ea878529ef4>\", line 1, in <module>\n    W = tf.Variable([.3], tf.float32)  # Variable adds learnable parameters in graph\n  File \"/home/hanul/anaconda2/lib/python2.7/site-packages/tensorflow/python/ops/variables.py\", line 213, in __init__\n    constraint=constraint)\n  File \"/home/hanul/anaconda2/lib/python2.7/site-packages/tensorflow/python/ops/variables.py\", line 356, in _init_from_args\n    self._snapshot = array_ops.identity(self._variable, name=\"read\")\n  File \"/home/hanul/anaconda2/lib/python2.7/site-packages/tensorflow/python/ops/array_ops.py\", line 125, in identity\n    return gen_array_ops.identity(input, name=name)\n  File \"/home/hanul/anaconda2/lib/python2.7/site-packages/tensorflow/python/ops/gen_array_ops.py\", line 2071, in identity\n    \"Identity\", input=input, name=name)\n  File \"/home/hanul/anaconda2/lib/python2.7/site-packages/tensorflow/python/framework/op_def_library.py\", line 787, in _apply_op_helper\n    op_def=op_def)\n  File \"/home/hanul/anaconda2/lib/python2.7/site-packages/tensorflow/python/framework/ops.py\", line 2956, in create_op\n    op_def=op_def)\n  File \"/home/hanul/anaconda2/lib/python2.7/site-packages/tensorflow/python/framework/ops.py\", line 1470, in __init__\n    self._traceback = self._graph._extract_stack()  # pylint: disable=protected-access\n\nFailedPreconditionError (see above for traceback): Attempting to use uninitialized value Variable\n\t [[Node: Variable/read = Identity[T=DT_FLOAT, _class=[\"loc:@Variable\"], _device=\"/job:localhost/replica:0/task:0/device:GPU:0\"](Variable)]]\n\t [[Node: add_1/_21 = _Recv[client_terminated=false, recv_device=\"/job:localhost/replica:0/task:0/device:CPU:0\", send_device=\"/job:localhost/replica:0/task:0/device:GPU:0\", send_device_incarnation=1, tensor_name=\"edge_12_add_1\", tensor_type=DT_FLOAT, _device=\"/job:localhost/replica:0/task:0/device:CPU:0\"]()]]\n",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mFailedPreconditionError\u001b[0m                   Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-9-cef581f4ca91>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;31m# without initializing variable, running graph causes error\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0;32mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msess\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrun\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlinear_model\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m{\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m3\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m4\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m/home/hanul/anaconda2/lib/python2.7/site-packages/tensorflow/python/client/session.pyc\u001b[0m in \u001b[0;36mrun\u001b[0;34m(self, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m    887\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    888\u001b[0m       result = self._run(None, fetches, feed_dict, options_ptr,\n\u001b[0;32m--> 889\u001b[0;31m                          run_metadata_ptr)\n\u001b[0m\u001b[1;32m    890\u001b[0m       \u001b[0;32mif\u001b[0m \u001b[0mrun_metadata\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    891\u001b[0m         \u001b[0mproto_data\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf_session\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTF_GetBuffer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrun_metadata_ptr\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/home/hanul/anaconda2/lib/python2.7/site-packages/tensorflow/python/client/session.pyc\u001b[0m in \u001b[0;36m_run\u001b[0;34m(self, handle, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m   1118\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mfinal_fetches\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0mfinal_targets\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mhandle\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mfeed_dict_tensor\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1119\u001b[0m       results = self._do_run(handle, final_targets, final_fetches,\n\u001b[0;32m-> 1120\u001b[0;31m                              feed_dict_tensor, options, run_metadata)\n\u001b[0m\u001b[1;32m   1121\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1122\u001b[0m       \u001b[0mresults\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/home/hanul/anaconda2/lib/python2.7/site-packages/tensorflow/python/client/session.pyc\u001b[0m in \u001b[0;36m_do_run\u001b[0;34m(self, handle, target_list, fetch_list, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m   1315\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mhandle\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1316\u001b[0m       return self._do_call(_run_fn, self._session, feeds, fetches, targets,\n\u001b[0;32m-> 1317\u001b[0;31m                            options, run_metadata)\n\u001b[0m\u001b[1;32m   1318\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1319\u001b[0m       \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_do_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0m_prun_fn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_session\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhandle\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeeds\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfetches\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/home/hanul/anaconda2/lib/python2.7/site-packages/tensorflow/python/client/session.pyc\u001b[0m in \u001b[0;36m_do_call\u001b[0;34m(self, fn, *args)\u001b[0m\n\u001b[1;32m   1334\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mKeyError\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1335\u001b[0m           \u001b[0;32mpass\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1336\u001b[0;31m       \u001b[0;32mraise\u001b[0m \u001b[0mtype\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnode_def\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mop\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmessage\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1337\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1338\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0m_extend_graph\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mFailedPreconditionError\u001b[0m: Attempting to use uninitialized value Variable\n\t [[Node: Variable/read = Identity[T=DT_FLOAT, _class=[\"loc:@Variable\"], _device=\"/job:localhost/replica:0/task:0/device:GPU:0\"](Variable)]]\n\t [[Node: add_1/_21 = _Recv[client_terminated=false, recv_device=\"/job:localhost/replica:0/task:0/device:CPU:0\", send_device=\"/job:localhost/replica:0/task:0/device:GPU:0\", send_device_incarnation=1, tensor_name=\"edge_12_add_1\", tensor_type=DT_FLOAT, _device=\"/job:localhost/replica:0/task:0/device:CPU:0\"]()]]\n\nCaused by op u'Variable/read', defined at:\n  File \"/home/hanul/anaconda2/lib/python2.7/runpy.py\", line 162, in _run_module_as_main\n    \"__main__\", fname, loader, pkg_name)\n  File \"/home/hanul/anaconda2/lib/python2.7/runpy.py\", line 72, in _run_code\n    exec code in run_globals\n  File \"/home/hanul/anaconda2/lib/python2.7/site-packages/ipykernel_launcher.py\", line 16, in <module>\n    app.launch_new_instance()\n  File \"/home/hanul/anaconda2/lib/python2.7/site-packages/traitlets/config/application.py\", line 658, in launch_instance\n    app.start()\n  File \"/home/hanul/anaconda2/lib/python2.7/site-packages/ipykernel/kernelapp.py\", line 477, in start\n    ioloop.IOLoop.instance().start()\n  File \"/home/hanul/anaconda2/lib/python2.7/site-packages/zmq/eventloop/ioloop.py\", line 177, in start\n    super(ZMQIOLoop, self).start()\n  File \"/home/hanul/anaconda2/lib/python2.7/site-packages/tornado/ioloop.py\", line 888, in start\n    handler_func(fd_obj, events)\n  File \"/home/hanul/anaconda2/lib/python2.7/site-packages/tornado/stack_context.py\", line 277, in null_wrapper\n    return fn(*args, **kwargs)\n  File \"/home/hanul/anaconda2/lib/python2.7/site-packages/zmq/eventloop/zmqstream.py\", line 440, in _handle_events\n    self._handle_recv()\n  File \"/home/hanul/anaconda2/lib/python2.7/site-packages/zmq/eventloop/zmqstream.py\", line 472, in _handle_recv\n    self._run_callback(callback, msg)\n  File \"/home/hanul/anaconda2/lib/python2.7/site-packages/zmq/eventloop/zmqstream.py\", line 414, in _run_callback\n    callback(*args, **kwargs)\n  File \"/home/hanul/anaconda2/lib/python2.7/site-packages/tornado/stack_context.py\", line 277, in null_wrapper\n    return fn(*args, **kwargs)\n  File \"/home/hanul/anaconda2/lib/python2.7/site-packages/ipykernel/kernelbase.py\", line 283, in dispatcher\n    return self.dispatch_shell(stream, msg)\n  File \"/home/hanul/anaconda2/lib/python2.7/site-packages/ipykernel/kernelbase.py\", line 235, in dispatch_shell\n    handler(stream, idents, msg)\n  File \"/home/hanul/anaconda2/lib/python2.7/site-packages/ipykernel/kernelbase.py\", line 399, in execute_request\n    user_expressions, allow_stdin)\n  File \"/home/hanul/anaconda2/lib/python2.7/site-packages/ipykernel/ipkernel.py\", line 196, in do_execute\n    res = shell.run_cell(code, store_history=store_history, silent=silent)\n  File \"/home/hanul/anaconda2/lib/python2.7/site-packages/ipykernel/zmqshell.py\", line 533, in run_cell\n    return super(ZMQInteractiveShell, self).run_cell(*args, **kwargs)\n  File \"/home/hanul/anaconda2/lib/python2.7/site-packages/IPython/core/interactiveshell.py\", line 2718, in run_cell\n    interactivity=interactivity, compiler=compiler, result=result)\n  File \"/home/hanul/anaconda2/lib/python2.7/site-packages/IPython/core/interactiveshell.py\", line 2822, in run_ast_nodes\n    if self.run_code(code, result):\n  File \"/home/hanul/anaconda2/lib/python2.7/site-packages/IPython/core/interactiveshell.py\", line 2882, in run_code\n    exec(code_obj, self.user_global_ns, self.user_ns)\n  File \"<ipython-input-8-5ea878529ef4>\", line 1, in <module>\n    W = tf.Variable([.3], tf.float32)  # Variable adds learnable parameters in graph\n  File \"/home/hanul/anaconda2/lib/python2.7/site-packages/tensorflow/python/ops/variables.py\", line 213, in __init__\n    constraint=constraint)\n  File \"/home/hanul/anaconda2/lib/python2.7/site-packages/tensorflow/python/ops/variables.py\", line 356, in _init_from_args\n    self._snapshot = array_ops.identity(self._variable, name=\"read\")\n  File \"/home/hanul/anaconda2/lib/python2.7/site-packages/tensorflow/python/ops/array_ops.py\", line 125, in identity\n    return gen_array_ops.identity(input, name=name)\n  File \"/home/hanul/anaconda2/lib/python2.7/site-packages/tensorflow/python/ops/gen_array_ops.py\", line 2071, in identity\n    \"Identity\", input=input, name=name)\n  File \"/home/hanul/anaconda2/lib/python2.7/site-packages/tensorflow/python/framework/op_def_library.py\", line 787, in _apply_op_helper\n    op_def=op_def)\n  File \"/home/hanul/anaconda2/lib/python2.7/site-packages/tensorflow/python/framework/ops.py\", line 2956, in create_op\n    op_def=op_def)\n  File \"/home/hanul/anaconda2/lib/python2.7/site-packages/tensorflow/python/framework/ops.py\", line 1470, in __init__\n    self._traceback = self._graph._extract_stack()  # pylint: disable=protected-access\n\nFailedPreconditionError (see above for traceback): Attempting to use uninitialized value Variable\n\t [[Node: Variable/read = Identity[T=DT_FLOAT, _class=[\"loc:@Variable\"], _device=\"/job:localhost/replica:0/task:0/device:GPU:0\"](Variable)]]\n\t [[Node: add_1/_21 = _Recv[client_terminated=false, recv_device=\"/job:localhost/replica:0/task:0/device:CPU:0\", send_device=\"/job:localhost/replica:0/task:0/device:GPU:0\", send_device_incarnation=1, tensor_name=\"edge_12_add_1\", tensor_type=DT_FLOAT, _device=\"/job:localhost/replica:0/task:0/device:CPU:0\"]()]]\n"
     ]
    }
   ],
   "source": [
    "# without initializing variable, running graph causes error\n",
    "print(sess.run(linear_model, {x:[1,2,3,4]}))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- global_variables_initializer로 graph에 정의된 모든 variable들을 초기화하고 다시 시도하면 성공."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# run variable initializer op, before running the graph\n",
    "init = tf.global_variables_initializer()\n",
    "sess.run(init)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[array([ 0.30000001], dtype=float32), array([-0.30000001], dtype=float32)]\n",
      "[ 0.          0.30000001  0.60000002  0.90000004]\n"
     ]
    }
   ],
   "source": [
    "# Then, it works\n",
    "print(sess.run([W, b]))\n",
    "print(sess.run(linear_model, {x:[1,2,3,4]}))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- loss 함수는 squared error function 사용"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.0\n",
      "1.69\n",
      "6.76\n",
      "15.21\n",
      "loss :  23.66\n"
     ]
    }
   ],
   "source": [
    "# Define loss\n",
    "y = tf.placeholder(tf.float32)\n",
    "squared_deltas = tf.square(linear_model - y)\n",
    "loss = tf.reduce_sum(squared_deltas)\n",
    "\n",
    "print(sess.run(loss, {x:[1], y:[0]}))\n",
    "print(sess.run(loss, {x:[2], y:[-1]}))\n",
    "print(sess.run(loss, {x:[3], y:[-2]}))\n",
    "print(sess.run(loss, {x:[4], y:[-3]}))\n",
    "print \"loss : \",sess.run(loss, {x:[1,2,3,4], y:[0,-1,-2,-3]})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- model을 training하기 전에, variable의 값 변경하는 법을 소개..\n",
    "- tf.assign(variable, modified_value) 하여 variable의 값을 modified_value로 바꾸는 연산자 생성하고\n",
    "- sess.run()에 넘겨 값을 바꾼다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Before assigning, W :  0.3 b :  -0.3\n",
      "After assigning, W :  -1.0 b :  1.0\n",
      "0.0\n"
     ]
    }
   ],
   "source": [
    "# assign different value to the variables\n",
    "fixW = tf.assign(W, [-1.])\n",
    "fixb = tf.assign(b, [1.])   # These are optimal parameters\n",
    "\n",
    "print \"Before assigning, W : \",sess.run(W)[0], \"b : \",sess.run(b)[0]\n",
    "sess.run([fixW, fixb])\n",
    "print \"After assigning, W : \",sess.run(W)[0], \"b : \",sess.run(b)[0]\n",
    "\n",
    "print(sess.run(loss, {x:[1,2,3,4], y:[0,-1,-2,-3]}))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Initializer을 다시 run하면 처음에 정의된 값으로 초기화."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ater initializing again, W :  0.3 b :  -0.3\n"
     ]
    }
   ],
   "source": [
    "# Go back to the initial value\n",
    "sess.run(init)\n",
    "print \"Ater initializing again, W : \",sess.run(W)[0], \"b : \",sess.run(b)[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- tf.train에 정의된 optimizer중 하나를 이용하여 variable들을 learning한다.\n",
    "- 예시에서는 일반적인 Stochastic Gradient Descent (SGD) optimizer 이용.\n",
    "- Optimizer는 loss를 minimizing하는 방향으로 배우도록 한다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[array([-0.52916396], dtype=float32), array([-0.38427448], dtype=float32)]\n",
      "iteration :  1  loss :  Tensor(\"Sum:0\", dtype=float32)\n",
      "[array([-0.53481066], dtype=float32), array([-0.36769974], dtype=float32)]\n",
      "iteration :  2  loss :  Tensor(\"Sum:0\", dtype=float32)\n",
      "[array([-0.54038435], dtype=float32), array([-0.35132164], dtype=float32)]\n",
      "iteration :  3  loss :  Tensor(\"Sum:0\", dtype=float32)\n",
      "[array([-0.54588938], dtype=float32), array([-0.33513904], dtype=float32)]\n",
      "iteration :  4  loss :  Tensor(\"Sum:0\", dtype=float32)\n",
      "[array([-0.55132794], dtype=float32), array([-0.31915003], dtype=float32)]\n",
      "iteration :  5  loss :  Tensor(\"Sum:0\", dtype=float32)\n",
      "[array([-0.55670118], dtype=float32), array([-0.30335245], dtype=float32)]\n",
      "iteration :  6  loss :  Tensor(\"Sum:0\", dtype=float32)\n",
      "[array([-0.56200999], dtype=float32), array([-0.28774402], dtype=float32)]\n",
      "iteration :  7  loss :  Tensor(\"Sum:0\", dtype=float32)\n",
      "[array([-0.5672552], dtype=float32), array([-0.27232251], dtype=float32)]\n",
      "iteration :  8  loss :  Tensor(\"Sum:0\", dtype=float32)\n",
      "[array([-0.57243758], dtype=float32), array([-0.25708568], dtype=float32)]\n",
      "iteration :  9  loss :  Tensor(\"Sum:0\", dtype=float32)\n",
      "[array([-0.57755792], dtype=float32), array([-0.24203131], dtype=float32)]\n",
      "iteration :  10  loss :  Tensor(\"Sum:0\", dtype=float32)\n",
      "[array([-0.58261693], dtype=float32), array([-0.22715722], dtype=float32)]\n",
      "iteration :  11  loss :  Tensor(\"Sum:0\", dtype=float32)\n",
      "[array([-0.58761531], dtype=float32), array([-0.21246126], dtype=float32)]\n",
      "iteration :  12  loss :  Tensor(\"Sum:0\", dtype=float32)\n",
      "[array([-0.59255385], dtype=float32), array([-0.1979413], dtype=float32)]\n",
      "iteration :  13  loss :  Tensor(\"Sum:0\", dtype=float32)\n",
      "[array([-0.59743327], dtype=float32), array([-0.18359523], dtype=float32)]\n",
      "iteration :  14  loss :  Tensor(\"Sum:0\", dtype=float32)\n",
      "[array([-0.60225427], dtype=float32), array([-0.16942096], dtype=float32)]\n",
      "iteration :  15  loss :  Tensor(\"Sum:0\", dtype=float32)\n",
      "[array([-0.60701752], dtype=float32), array([-0.15541643], dtype=float32)]\n",
      "iteration :  16  loss :  Tensor(\"Sum:0\", dtype=float32)\n",
      "[array([-0.61172372], dtype=float32), array([-0.14157961], dtype=float32)]\n",
      "iteration :  17  loss :  Tensor(\"Sum:0\", dtype=float32)\n",
      "[array([-0.61637354], dtype=float32), array([-0.1279085], dtype=float32)]\n",
      "iteration :  18  loss :  Tensor(\"Sum:0\", dtype=float32)\n",
      "[array([-0.62096775], dtype=float32), array([-0.11440111], dtype=float32)]\n",
      "iteration :  19  loss :  Tensor(\"Sum:0\", dtype=float32)\n",
      "[array([-0.62550688], dtype=float32), array([-0.10105547], dtype=float32)]\n",
      "iteration :  20  loss :  Tensor(\"Sum:0\", dtype=float32)\n",
      "[array([-0.62999165], dtype=float32), array([-0.08786966], dtype=float32)]\n",
      "iteration :  21  loss :  Tensor(\"Sum:0\", dtype=float32)\n",
      "[array([-0.63442272], dtype=float32), array([-0.07484176], dtype=float32)]\n",
      "iteration :  22  loss :  Tensor(\"Sum:0\", dtype=float32)\n",
      "[array([-0.63880074], dtype=float32), array([-0.06196988], dtype=float32)]\n",
      "iteration :  23  loss :  Tensor(\"Sum:0\", dtype=float32)\n",
      "[array([-0.64312631], dtype=float32), array([-0.04925214], dtype=float32)]\n",
      "iteration :  24  loss :  Tensor(\"Sum:0\", dtype=float32)\n",
      "[array([-0.64740008], dtype=float32), array([-0.03668671], dtype=float32)]\n",
      "iteration :  25  loss :  Tensor(\"Sum:0\", dtype=float32)\n",
      "[array([-0.65162271], dtype=float32), array([-0.02427176], dtype=float32)]\n",
      "iteration :  26  loss :  Tensor(\"Sum:0\", dtype=float32)\n",
      "[array([-0.65579474], dtype=float32), array([-0.01200548], dtype=float32)]\n",
      "iteration :  27  loss :  Tensor(\"Sum:0\", dtype=float32)\n",
      "[array([-0.65991682], dtype=float32), array([ 0.00011391], dtype=float32)]\n",
      "iteration :  28  loss :  Tensor(\"Sum:0\", dtype=float32)\n",
      "[array([-0.66398954], dtype=float32), array([ 0.01208815], dtype=float32)]\n",
      "iteration :  29  loss :  Tensor(\"Sum:0\", dtype=float32)\n",
      "[array([-0.66801345], dtype=float32), array([ 0.02391901], dtype=float32)]\n",
      "iteration :  30  loss :  Tensor(\"Sum:0\", dtype=float32)\n",
      "[array([-0.6719892], dtype=float32), array([ 0.03560818], dtype=float32)]\n",
      "iteration :  31  loss :  Tensor(\"Sum:0\", dtype=float32)\n",
      "[array([-0.67591733], dtype=float32), array([ 0.04715736], dtype=float32)]\n",
      "iteration :  32  loss :  Tensor(\"Sum:0\", dtype=float32)\n",
      "[array([-0.67979842], dtype=float32), array([ 0.05856823], dtype=float32)]\n",
      "iteration :  33  loss :  Tensor(\"Sum:0\", dtype=float32)\n",
      "[array([-0.68363303], dtype=float32), array([ 0.06984246], dtype=float32)]\n",
      "iteration :  34  loss :  Tensor(\"Sum:0\", dtype=float32)\n",
      "[array([-0.68742174], dtype=float32), array([ 0.08098166], dtype=float32)]\n",
      "iteration :  35  loss :  Tensor(\"Sum:0\", dtype=float32)\n",
      "[array([-0.69116503], dtype=float32), array([ 0.09198748], dtype=float32)]\n",
      "iteration :  36  loss :  Tensor(\"Sum:0\", dtype=float32)\n",
      "[array([-0.6948635], dtype=float32), array([ 0.10286149], dtype=float32)]\n",
      "iteration :  37  loss :  Tensor(\"Sum:0\", dtype=float32)\n",
      "[array([-0.69851768], dtype=float32), array([ 0.11360527], dtype=float32)]\n",
      "iteration :  38  loss :  Tensor(\"Sum:0\", dtype=float32)\n",
      "[array([-0.70212811], dtype=float32), array([ 0.12422038], dtype=float32)]\n",
      "iteration :  39  loss :  Tensor(\"Sum:0\", dtype=float32)\n",
      "[array([-0.70569533], dtype=float32), array([ 0.13470837], dtype=float32)]\n",
      "iteration :  40  loss :  Tensor(\"Sum:0\", dtype=float32)\n",
      "[array([-0.70921981], dtype=float32), array([ 0.14507076], dtype=float32)]\n",
      "iteration :  41  loss :  Tensor(\"Sum:0\", dtype=float32)\n",
      "[array([-0.7127021], dtype=float32), array([ 0.15530905], dtype=float32)]\n",
      "iteration :  42  loss :  Tensor(\"Sum:0\", dtype=float32)\n",
      "[array([-0.71614265], dtype=float32), array([ 0.16542475], dtype=float32)]\n",
      "iteration :  43  loss :  Tensor(\"Sum:0\", dtype=float32)\n",
      "[array([-0.71954203], dtype=float32), array([ 0.1754193], dtype=float32)]\n",
      "iteration :  44  loss :  Tensor(\"Sum:0\", dtype=float32)\n",
      "[array([-0.72290069], dtype=float32), array([ 0.18529417], dtype=float32)]\n",
      "iteration :  45  loss :  Tensor(\"Sum:0\", dtype=float32)\n",
      "[array([-0.72621912], dtype=float32), array([ 0.19505078], dtype=float32)]\n",
      "iteration :  46  loss :  Tensor(\"Sum:0\", dtype=float32)\n",
      "[array([-0.72949779], dtype=float32), array([ 0.20469053], dtype=float32)]\n",
      "iteration :  47  loss :  Tensor(\"Sum:0\", dtype=float32)\n",
      "[array([-0.73273724], dtype=float32), array([ 0.21421485], dtype=float32)]\n",
      "iteration :  48  loss :  Tensor(\"Sum:0\", dtype=float32)\n",
      "[array([-0.73593783], dtype=float32), array([ 0.22362511], dtype=float32)]\n",
      "iteration :  49  loss :  Tensor(\"Sum:0\", dtype=float32)\n",
      "[array([-0.73910016], dtype=float32), array([ 0.23292267], dtype=float32)]\n",
      "iteration :  50  loss :  Tensor(\"Sum:0\", dtype=float32)\n",
      "[array([-0.74222463], dtype=float32), array([ 0.24210888], dtype=float32)]\n",
      "iteration :  51  loss :  Tensor(\"Sum:0\", dtype=float32)\n",
      "[array([-0.74531162], dtype=float32), array([ 0.25118509], dtype=float32)]\n",
      "iteration :  52  loss :  Tensor(\"Sum:0\", dtype=float32)\n",
      "[array([-0.74836165], dtype=float32), array([ 0.26015261], dtype=float32)]\n",
      "iteration :  53  loss :  Tensor(\"Sum:0\", dtype=float32)\n",
      "[array([-0.7513752], dtype=float32), array([ 0.26901272], dtype=float32)]\n",
      "iteration :  54  loss :  Tensor(\"Sum:0\", dtype=float32)\n",
      "[array([-0.75435263], dtype=float32), array([ 0.27776673], dtype=float32)]\n",
      "iteration :  55  loss :  Tensor(\"Sum:0\", dtype=float32)\n",
      "[array([-0.75729442], dtype=float32), array([ 0.28641593], dtype=float32)]\n",
      "iteration :  56  loss :  Tensor(\"Sum:0\", dtype=float32)\n",
      "[array([-0.76020098], dtype=float32), array([ 0.29496154], dtype=float32)]\n",
      "iteration :  57  loss :  Tensor(\"Sum:0\", dtype=float32)\n",
      "[array([-0.76307267], dtype=float32), array([ 0.30340481], dtype=float32)]\n",
      "iteration :  58  loss :  Tensor(\"Sum:0\", dtype=float32)\n",
      "[array([-0.76591003], dtype=float32), array([ 0.31174695], dtype=float32)]\n",
      "iteration :  59  loss :  Tensor(\"Sum:0\", dtype=float32)\n",
      "[array([-0.76871341], dtype=float32), array([ 0.3199892], dtype=float32)]\n",
      "iteration :  60  loss :  Tensor(\"Sum:0\", dtype=float32)\n",
      "[array([-0.77148318], dtype=float32), array([ 0.32813275], dtype=float32)]\n",
      "iteration :  61  loss :  Tensor(\"Sum:0\", dtype=float32)\n",
      "[array([-0.77421981], dtype=float32), array([ 0.33617878], dtype=float32)]\n",
      "iteration :  62  loss :  Tensor(\"Sum:0\", dtype=float32)\n",
      "[array([-0.77692366], dtype=float32), array([ 0.34412843], dtype=float32)]\n",
      "iteration :  63  loss :  Tensor(\"Sum:0\", dtype=float32)\n",
      "[array([-0.77959514], dtype=float32), array([ 0.35198289], dtype=float32)]\n",
      "iteration :  64  loss :  Tensor(\"Sum:0\", dtype=float32)\n",
      "[array([-0.78223461], dtype=float32), array([ 0.3597433], dtype=float32)]\n",
      "iteration :  65  loss :  Tensor(\"Sum:0\", dtype=float32)\n",
      "[array([-0.78484249], dtype=float32), array([ 0.36741075], dtype=float32)]\n",
      "iteration :  66  loss :  Tensor(\"Sum:0\", dtype=float32)\n",
      "[array([-0.78741914], dtype=float32), array([ 0.37498638], dtype=float32)]\n",
      "iteration :  67  loss :  Tensor(\"Sum:0\", dtype=float32)\n",
      "[array([-0.78996491], dtype=float32), array([ 0.38247129], dtype=float32)]\n",
      "iteration :  68  loss :  Tensor(\"Sum:0\", dtype=float32)\n",
      "[array([-0.79248023], dtype=float32), array([ 0.38986656], dtype=float32)]\n",
      "iteration :  69  loss :  Tensor(\"Sum:0\", dtype=float32)\n",
      "[array([-0.79496539], dtype=float32), array([ 0.39717329], dtype=float32)]\n",
      "iteration :  70  loss :  Tensor(\"Sum:0\", dtype=float32)\n",
      "[array([-0.7974208], dtype=float32), array([ 0.40439251], dtype=float32)]\n",
      "iteration :  71  loss :  Tensor(\"Sum:0\", dtype=float32)\n",
      "[array([-0.79984683], dtype=float32), array([ 0.41152528], dtype=float32)]\n",
      "iteration :  72  loss :  Tensor(\"Sum:0\", dtype=float32)\n",
      "[array([-0.80224377], dtype=float32), array([ 0.41857263], dtype=float32)]\n",
      "iteration :  73  loss :  Tensor(\"Sum:0\", dtype=float32)\n",
      "[array([-0.80461204], dtype=float32), array([ 0.42553559], dtype=float32)]\n",
      "iteration :  74  loss :  Tensor(\"Sum:0\", dtype=float32)\n",
      "[array([-0.80695194], dtype=float32), array([ 0.43241516], dtype=float32)]\n",
      "iteration :  75  loss :  Tensor(\"Sum:0\", dtype=float32)\n",
      "[array([-0.80926383], dtype=float32), array([ 0.43921232], dtype=float32)]\n",
      "iteration :  76  loss :  Tensor(\"Sum:0\", dtype=float32)\n",
      "[array([-0.81154799], dtype=float32), array([ 0.4459281], dtype=float32)]\n",
      "iteration :  77  loss :  Tensor(\"Sum:0\", dtype=float32)\n",
      "[array([-0.81380481], dtype=float32), array([ 0.45256343], dtype=float32)]\n",
      "iteration :  78  loss :  Tensor(\"Sum:0\", dtype=float32)\n",
      "[array([-0.81603462], dtype=float32), array([ 0.45911932], dtype=float32)]\n",
      "iteration :  79  loss :  Tensor(\"Sum:0\", dtype=float32)\n",
      "[array([-0.81823772], dtype=float32), array([ 0.46559671], dtype=float32)]\n",
      "iteration :  80  loss :  Tensor(\"Sum:0\", dtype=float32)\n",
      "[array([-0.82041442], dtype=float32), array([ 0.47199652], dtype=float32)]\n",
      "iteration :  81  loss :  Tensor(\"Sum:0\", dtype=float32)\n",
      "[array([-0.82256508], dtype=float32), array([ 0.47831967], dtype=float32)]\n",
      "iteration :  82  loss :  Tensor(\"Sum:0\", dtype=float32)\n",
      "[array([-0.82468998], dtype=float32), array([ 0.48456711], dtype=float32)]\n",
      "iteration :  83  loss :  Tensor(\"Sum:0\", dtype=float32)\n",
      "[array([-0.82678944], dtype=float32), array([ 0.49073973], dtype=float32)]\n",
      "iteration :  84  loss :  Tensor(\"Sum:0\", dtype=float32)\n",
      "[array([-0.82886374], dtype=float32), array([ 0.49683845], dtype=float32)]\n",
      "iteration :  85  loss :  Tensor(\"Sum:0\", dtype=float32)\n",
      "[array([-0.83091319], dtype=float32), array([ 0.50286412], dtype=float32)]\n",
      "iteration :  86  loss :  Tensor(\"Sum:0\", dtype=float32)\n",
      "[array([-0.83293808], dtype=float32), array([ 0.50881761], dtype=float32)]\n",
      "iteration :  87  loss :  Tensor(\"Sum:0\", dtype=float32)\n",
      "[array([-0.83493876], dtype=float32), array([ 0.51469982], dtype=float32)]\n",
      "iteration :  88  loss :  Tensor(\"Sum:0\", dtype=float32)\n",
      "[array([-0.83691549], dtype=float32), array([ 0.52051157], dtype=float32)]\n",
      "iteration :  89  loss :  Tensor(\"Sum:0\", dtype=float32)\n",
      "[array([-0.8388685], dtype=float32), array([ 0.52625376], dtype=float32)]\n",
      "iteration :  90  loss :  Tensor(\"Sum:0\", dtype=float32)\n",
      "[array([-0.84079814], dtype=float32), array([ 0.53192717], dtype=float32)]\n",
      "iteration :  91  loss :  Tensor(\"Sum:0\", dtype=float32)\n",
      "[array([-0.84270465], dtype=float32), array([ 0.53753263], dtype=float32)]\n",
      "iteration :  92  loss :  Tensor(\"Sum:0\", dtype=float32)\n",
      "[array([-0.8445884], dtype=float32), array([ 0.54307097], dtype=float32)]\n",
      "iteration :  93  loss :  Tensor(\"Sum:0\", dtype=float32)\n",
      "[array([-0.84644955], dtype=float32), array([ 0.54854298], dtype=float32)]\n",
      "iteration :  94  loss :  Tensor(\"Sum:0\", dtype=float32)\n",
      "[array([-0.84828842], dtype=float32), array([ 0.55394948], dtype=float32)]\n",
      "iteration :  95  loss :  Tensor(\"Sum:0\", dtype=float32)\n",
      "[array([-0.85010529], dtype=float32), array([ 0.55929118], dtype=float32)]\n",
      "iteration :  96  loss :  Tensor(\"Sum:0\", dtype=float32)\n",
      "[array([-0.85190034], dtype=float32), array([ 0.56456894], dtype=float32)]\n",
      "iteration :  97  loss :  Tensor(\"Sum:0\", dtype=float32)\n",
      "[array([-0.85367393], dtype=float32), array([ 0.56978351], dtype=float32)]\n",
      "iteration :  98  loss :  Tensor(\"Sum:0\", dtype=float32)\n",
      "[array([-0.85542625], dtype=float32), array([ 0.57493562], dtype=float32)]\n",
      "iteration :  99  loss :  Tensor(\"Sum:0\", dtype=float32)\n",
      "[array([-0.85715765], dtype=float32), array([ 0.58002603], dtype=float32)]\n",
      "iteration :  100  loss :  Tensor(\"Sum:0\", dtype=float32)\n",
      "iteration :  100  loss :  Tensor(\"Sum:0\", dtype=float32)\n",
      "Learned model parameters, \t W :  -0.858868 , b :  0.585055\n",
      "Optimal parameters are,   \t W :  -1 \t, b :  1\n"
     ]
    }
   ],
   "source": [
    "# Learn the model using optimizer\n",
    "optimizer = tf.train.GradientDescentOptimizer(0.01)\n",
    "train = optimizer.minimize(loss)\n",
    "\n",
    "for i in range(100):\n",
    "    print(sess.run([W, b]))\n",
    "    print \"iteration : \", i+1 ,\" loss : \", loss\n",
    "    _, l = sess.run([train, loss], {x:[1,2,3,4], y:[0,-1,-2,-3]})\n",
    "    if (i+1) % 100 == 0:\n",
    "        print \"iteration : \", i+1 ,\" loss : \", loss\n",
    "    \n",
    "print \"Learned model parameters, \\t W : \",sess.run(W)[0],\", b : \",sess.run(b)[0]\n",
    "print \"Optimal parameters are,   \\t W :  -1 \\t, b :  1\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "### 2.2. Initializing Variables\n",
    "- 여러가지 initialize 방법들 소개\n",
    "\n",
    "#### 2.2.1. tf.global_variable_initializer()\n",
    "- model의 모든 variable 초기화"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "W1 : 0.2 ,\t W2 : 0.4 ,\t b : -0.3\n"
     ]
    }
   ],
   "source": [
    "W1 = tf.Variable([.2], tf.float32)\n",
    "W2 = tf.Variable([.4], tf.float32)\n",
    "b = tf.Variable([-.3], tf.float32)\n",
    "\n",
    "init = tf.global_variables_initializer()\n",
    "sess.run(init)\n",
    "print \"W1 :\",sess.run(W1)[0],\",\\t W2 :\",sess.run(W2)[0],\",\\t b :\",sess.run(b)[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2.2.2 tf.variables_initialize()\n",
    "- tf.variables_initialize(var_list)는 var_list (variables들의 list)에 포함된 variables들만 initialize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "W1 : 0.1 ,\t W2 : 0.1 ,\t b : -0.1\n"
     ]
    }
   ],
   "source": [
    "# 위에 정의된 variable들의 값을 바꾼다.\n",
    "assn_W1 = W1.assign([.1])\n",
    "assn_W2 = W2.assign([.1])\n",
    "assn_b = b.assign([-.1])\n",
    "\n",
    "sess.run([assn_W1, assn_W2, assn_b])\n",
    "print \"W1 :\",sess.run(W1)[0],\",\\t W2 :\",sess.run(W2)[0],\",\\t b :\",sess.run(b)[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "W1 : 0.2 ,\t W2 : 0.1 ,\t b : -0.3\n"
     ]
    }
   ],
   "source": [
    "# W1, b만 초기값으로 initialize한다.\n",
    "init_W = tf.variables_initializer([W1, b])\n",
    "# init_W = tf.variables_initializer(set(tf.global_variables())-W2)\n",
    "sess.run(init_W)\n",
    "print \"W1 :\",sess.run(W1)[0],\",\\t W2 :\",sess.run(W2)[0],\",\\t b :\",sess.run(b)[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2.2.3 Variable.initialized_value()\n",
    "- 두 변수 A, B를 동일한 값으로 초기화시킬 때, Variable의 initialized_value()를 이용한다.\n",
    "- initialized_value는 그 변수의 초기화 값을 return한다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "A :\n",
      "[[ 0.1773243  -0.79489833  0.00858787]\n",
      " [-0.3675245  -0.14506102  0.55365795]] \n",
      "B :\n",
      "[[ 0.1773243  -0.79489833  0.00858787]\n",
      " [-0.3675245  -0.14506102  0.55365795]] \n",
      "C :\n",
      "[[ 0.35464859 -1.58979666  0.01717574]\n",
      " [-0.73504901 -0.29012203  1.1073159 ]]\n"
     ]
    }
   ],
   "source": [
    "A = tf.Variable(tf.random_normal([2,3], stddev=0.35), name=\"A\")\n",
    "# A와 같은 초기 값을 가지는 변수 B를 만든다.\n",
    "B = tf.Variable(A.initialized_value(), name=\"B\")\n",
    "# A의 초기값의 정확히 2배의 초기값을 가지는 변수 C를 만든다.\n",
    "C = tf.Variable(A.initialized_value() * 2.0, name=\"C\")\n",
    "\n",
    "init_ABC = tf.variables_initializer(set([A,B,C]))\n",
    "sess.run(init_ABC)\n",
    "print \"A :\\n\",sess.run(A),\"\\nB :\\n\",sess.run(B),\"\\nC :\\n\",sess.run(C)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.4 Saving and Loading Variables\n",
    "- 배운 모델을 저장하거나, 배워진 모델을 읽어들일 때 필요한 과정.\n",
    "- tf.train.Saver()을 사용한다.\n",
    "- 모든 변수가 아니라, 지정한 몇 개의 변수만 저장하고 불러올 수도 있다.\n",
    "- key : 저장/불러올 변수 이름의 이름, value : 저장/불러올 변수 를 가지는 python dictionary를 만들어서 train.Saver의 input으로 넘긴다.\n",
    "- train.Saver의 input이 없을 때는 default로 graph 내의 모든 변수들을 save/restore한다. \n",
    "- graph 내의 모든 변수들을 보려면 global_variables() 함수를 사용하여, print(global_bvariables()) 등을 이용.\n",
    "\n",
    "#### 2.4.1 Save model : all variables\n",
    "- tf.train.Saver.save()를 사용."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# clear all created variables, close session and reopen\n",
    "tf.reset_default_graph()\n",
    "sess.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "iteration :  20  loss :  15.2405\n",
      "iteration :  40  loss :  14.356\n",
      "iteration :  60  loss :  13.4779\n",
      "iteration :  80  loss :  12.6072\n",
      "iteration :  100  loss :  11.7457\n",
      "iteration :  120  loss :  10.8952\n",
      "iteration :  140  loss :  10.0583\n",
      "iteration :  160  loss :  9.23827\n",
      "iteration :  180  loss :  8.43942\n",
      "iteration :  200  loss :  7.66734\n",
      "v1 :\n",
      "[[ 0.30854711 -0.32130253]\n",
      " [-0.57005471 -0.27217346]] \n",
      "v2 :\n",
      "[[-1.59077775]\n",
      " [ 2.97532773]]\n",
      "Model (variables v1, v2) saved in file: ./model.ckpt\n"
     ]
    }
   ],
   "source": [
    "# Variable 생성\n",
    "v1 = tf.Variable(tf.random_normal([2,2]), name=\"v1\")\n",
    "v2 = tf.Variable(tf.random_normal([2,1]), name=\"v2\")\n",
    "x = tf.placeholder(tf.float32)\n",
    "y = tf.placeholder(tf.float32)\n",
    "\n",
    "init_op = tf.global_variables_initializer()\n",
    "\n",
    "# tf.train.Saver() 을 먼저 생성. 저장할 변수들 pass \n",
    "saver = tf.train.Saver({\"w1\": v1, \"w2\": v2})\n",
    "# saver = tf.train.Saver([v1, v2])   # name은 default(v.op.name)으로 들어감\n",
    "# saver = tf.train.Saver({v.op.name: v for v in [v1, v2]})\n",
    "\n",
    "# 모델 생성, 변수 초기화, training, 모델 저장.\n",
    "with tf.Session() as sess:\n",
    "    # model 생성\n",
    "    loss = tf.norm(tf.matmul(v1,x)+v2-y)\n",
    "    \n",
    "    # 변수 초기화\n",
    "    sess.run(init_op)\n",
    "    \n",
    "    # training\n",
    "    optimizer = tf.train.GradientDescentOptimizer(0.001)\n",
    "    train = optimizer.minimize(loss)\n",
    "    \n",
    "    for i in range(200):\n",
    "        _, l = sess.run([train, loss], {x:[[1.,2.,3.,4.],[0.,1.,2.,3.]], y:[[0.,-1.,-2.,-3.],[-1.,-2.,-3.,-4.]]})\n",
    "        if (i+1) % 20 == 0:\n",
    "            print \"iteration : \", i+1 ,\" loss : \", l\n",
    "    \n",
    "    # 디스크에 변수 저장\n",
    "    save_path = saver.save(sess, \"./model.ckpt\")  # 세션과 저장 파일 이름 지정\n",
    "    print \"v1 :\\n\",sess.run(v1), \"\\nv2 :\\n\", sess.run(v2)\n",
    "    print(\"Model (variables v1, v2) saved in file: %s\" % save_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2.4.2 Load model : all variables\n",
    "- 변수들을 저장된 값을 불러 초기화한다. \n",
    "- 이 경우 restore된 변수들은 따로 initializer로 초기화할 필요 없다.\n",
    "- tf.train.Saver.restore()를 사용\n",
    "- 밑의 예시는 위 예시에서 저장된 checkpoint부터 시작하여 training을 이어나가는 과정."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# clear all created variables, close session and reopen\n",
    "tf.reset_default_graph()\n",
    "sess.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Restoring parameters from ./model.ckpt\n",
      "Model restored.\n",
      "w1 :\n",
      "[[ 0.30854711 -0.32130253]\n",
      " [-0.57005471 -0.27217346]] \n",
      "w2 :\n",
      "[[-1.59077775]\n",
      " [ 2.97532773]]\n",
      "iteration :  20  loss :  6.92932\n",
      "iteration :  40  loss :  6.23469\n",
      "iteration :  60  loss :  5.59491\n",
      "iteration :  80  loss :  5.02306\n",
      "iteration :  100  loss :  4.53204\n",
      "iteration :  120  loss :  4.1311\n",
      "iteration :  140  loss :  3.82185\n",
      "iteration :  160  loss :  3.59613\n",
      "iteration :  180  loss :  3.43816\n",
      "iteration :  200  loss :  3.32957\n"
     ]
    }
   ],
   "source": [
    "# Variable 생성\n",
    "w1 = tf.Variable(tf.random_normal([2,2]), name=\"w1\")\n",
    "w2 = tf.Variable(tf.random_normal([2,1]), name=\"w2\")\n",
    "x = tf.placeholder(tf.float32)\n",
    "y = tf.placeholder(tf.float32)\n",
    "\n",
    "# restorer를 정의하여 restore할 \n",
    "restorer = tf.train.Saver({\"w1\": w1, \"w2\": w2})\n",
    "with tf.Session() as sess:\n",
    "    # model 생성\n",
    "    loss = tf.norm(tf.matmul(w1,x)+w2-y)\n",
    "    \n",
    "    # checkpoint 파일로부터 변수값 읽어와서 변수 초기화\n",
    "    restorer.restore(sess, \"./model.ckpt\")\n",
    "    print(\"Model restored.\")\n",
    "    print \"w1 :\\n\",sess.run(w1), \"\\nw2 :\\n\", sess.run(w2)\n",
    "    \n",
    "    # training\n",
    "    optimizer = tf.train.GradientDescentOptimizer(0.001)\n",
    "    train = optimizer.minimize(loss)\n",
    "    \n",
    "    for i in range(200):\n",
    "        _, l = sess.run([train, loss], {x:[[1.,2.,3.,4.],[0.,1.,2.,3.]], y:[[0.,-1.,-2.,-3.],[-1.,-2.,-3.,-4.]]})\n",
    "        if (i+1) % 20 == 0:\n",
    "            print \"iteration : \", i+1 ,\" loss : \", l\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- restore되지 않은 변수는 꼭 초기화를 해준다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# clear all created variables, close session and reopen\n",
    "tf.reset_default_graph()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Restoring parameters from model.ckpt\n",
      "Initialize w1, w2 by restoring saved values\n",
      "w1 :  [[ 0.30854711 -0.32130253]\n",
      " [-0.57005471 -0.27217346]] \n",
      "w2 :  [[-1.59077775]\n",
      " [ 2.97532773]]\n",
      "\n",
      "w3 :  [[ 1.00396395]\n",
      " [ 0.40798518]] \n",
      "w4 :  [[ 0.59372914]\n",
      " [-1.46765149]]\n"
     ]
    }
   ],
   "source": [
    "w1 = tf.Variable(tf.random_normal([2,2]), name=\"w1\")\n",
    "w2 = tf.Variable(tf.random_normal([2,1]), name=\"w2\")\n",
    "w3 = tf.Variable(tf.random_normal([2,1]), name=\"w3\")\n",
    "w4 = tf.Variable(tf.random_normal([2,1]), name=\"w4\")\n",
    "\n",
    "restorer = tf.train.Saver({\"w1\": w1, \"w2\": w2})\n",
    "with tf.Session() as sess:\n",
    "    restorer.restore(sess, \"model.ckpt\")\n",
    "    print(\"Initialize w1, w2 by restoring saved values\")\n",
    "\n",
    "    # restore 되지 않은 w3는 초기화\n",
    "    init_34 = tf.variables_initializer([w3, w4])\n",
    "    sess.run(init_34)\n",
    "    \n",
    "    print \"w1 : \",sess.run(w1), \"\\nw2 : \", sess.run(w2)\n",
    "    print \"\\nw3 : \",sess.run(w3), \"\\nw4 : \", sess.run(w4)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Sharing Variables\n",
    "- Variable은 선언할 때 마다 새로 생성되어 덮어씌어진다는 특징이 있는데, 이미 만든 variable은 새로 만들지 않고 기존의 것을 재사용하게 하도록 하는 방법이 필요할 때가 있다.\n",
    "- 밑의 예시는 convolution layer를 포함한 model인데, 두가지의 문제점이 있다.\n",
    "    - 첫째로, 함수 내의 모델이 고정되어있어 모델의 layer를 추가하려면 새로운함수를 만들거나, main함수에 직접 넣어주어야 하는 비효율이 발생한다.\n",
    "    - 둘째로, 모델에 input을 넣어줄 때마다 새로운 variable이 생성되어, input에 따라 다른 filter를 통과하게."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def my_image_filter(input_images):\n",
    "    conv1_weights = tf.Variable(tf.random_normal([5, 5, 3, 32]), name=\"conv1_weights\")\n",
    "    conv1_biases = tf.Variable(tf.zeros([32]), name=\"conv1_biases\")\n",
    "    conv1 = tf.nn.conv2d(input_images, conv1_weights, strides=[1, 1, 1, 1], padding='SAME')\n",
    "    relu1 = tf.nn.relu(conv1 + conv1_biases)\n",
    "\n",
    "    conv2_weights = tf.Variable(tf.random_normal([5, 5, 32, 32]), name=\"conv2_weights\")\n",
    "    conv2_biases = tf.Variable(tf.zeros([32]), name=\"conv2_biases\")\n",
    "    conv2 = tf.nn.conv2d(relu1, conv2_weights,  strides=[1, 1, 1, 1], padding='SAME')\n",
    "    return tf.nn.relu(conv2 + conv2_biases)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from scipy import misc\n",
    "\n",
    "image1 = tf.expand_dims(tf.constant(misc.imread('acoustic-guitar-player.jpg'), tf.float32), 0)\n",
    "image2 = tf.expand_dims(tf.constant(misc.imread('iris.jpg'), tf.float32), 0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- 위에서 언급한 두 번째 문제 발생"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "temp = tf.global_variables()\n",
    "# First call creates one set of 4 variables.\n",
    "result1 = my_image_filter(image1)\n",
    "print \"Number of created variables : \\n\",len(set(tf.global_variables())-set(temp))\n",
    "\n",
    "# Another set of 4 variables is created in the second call.\n",
    "result2 = my_image_filter(image2)\n",
    "print \"Number of created variables : \\n\",len(set(tf.global_variables())-set(temp))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- tf.placeholder를 이용하여 sess.run 할때마다 input을 넘겨주거나, variable들을 dictionary로 따로 함수 밖에 만들어놓은 뒤 함수에서 refer하면 두 번째 문제는 해결되지만, 첫번째 문제는 여전히 해결하지 못한다.\n",
    "- tf.variable_scope 와 tf.get_variable을 이용하여 두 문제를 효율적으로 해결해보자."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.1. tf.get_variable() & tf.variable_scope()\n",
    "- get_variable은 이미 존재하는 이름의 변수는 더 이상 만들지 않는다. \n",
    "- 같은 이름의 변수를 만들려고 하면 error가 발생\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "Variable v already exists, disallowed. Did you mean to set reuse=True or reuse=tf.AUTO_REUSE in VarScope? Originally defined at:\n\n  File \"<ipython-input-28-ed56b01b144e>\", line 1, in <module>\n    v = tf.get_variable(\"v\", [1])\n  File \"/home/hanul/anaconda2/lib/python2.7/site-packages/IPython/core/interactiveshell.py\", line 2882, in run_code\n    exec(code_obj, self.user_global_ns, self.user_ns)\n  File \"/home/hanul/anaconda2/lib/python2.7/site-packages/IPython/core/interactiveshell.py\", line 2822, in run_ast_nodes\n    if self.run_code(code, result):\n",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-28-ed56b01b144e>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0mv\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_variable\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"v\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mv1\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_variable\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"v\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m/home/hanul/anaconda2/lib/python2.7/site-packages/tensorflow/python/ops/variable_scope.pyc\u001b[0m in \u001b[0;36mget_variable\u001b[0;34m(name, shape, dtype, initializer, regularizer, trainable, collections, caching_device, partitioner, validate_shape, use_resource, custom_getter, constraint)\u001b[0m\n\u001b[1;32m   1201\u001b[0m       \u001b[0mpartitioner\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mpartitioner\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvalidate_shape\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mvalidate_shape\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1202\u001b[0m       \u001b[0muse_resource\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0muse_resource\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcustom_getter\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcustom_getter\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1203\u001b[0;31m       constraint=constraint)\n\u001b[0m\u001b[1;32m   1204\u001b[0m get_variable_or_local_docstring = (\n\u001b[1;32m   1205\u001b[0m     \"\"\"%s\n",
      "\u001b[0;32m/home/hanul/anaconda2/lib/python2.7/site-packages/tensorflow/python/ops/variable_scope.pyc\u001b[0m in \u001b[0;36mget_variable\u001b[0;34m(self, var_store, name, shape, dtype, initializer, regularizer, reuse, trainable, collections, caching_device, partitioner, validate_shape, use_resource, custom_getter, constraint)\u001b[0m\n\u001b[1;32m   1090\u001b[0m           \u001b[0mpartitioner\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mpartitioner\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvalidate_shape\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mvalidate_shape\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1091\u001b[0m           \u001b[0muse_resource\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0muse_resource\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcustom_getter\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcustom_getter\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1092\u001b[0;31m           constraint=constraint)\n\u001b[0m\u001b[1;32m   1093\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1094\u001b[0m   def _get_partitioned_variable(self,\n",
      "\u001b[0;32m/home/hanul/anaconda2/lib/python2.7/site-packages/tensorflow/python/ops/variable_scope.pyc\u001b[0m in \u001b[0;36mget_variable\u001b[0;34m(self, name, shape, dtype, initializer, regularizer, reuse, trainable, collections, caching_device, partitioner, validate_shape, use_resource, custom_getter, constraint)\u001b[0m\n\u001b[1;32m    423\u001b[0m           \u001b[0mcaching_device\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcaching_device\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpartitioner\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mpartitioner\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    424\u001b[0m           \u001b[0mvalidate_shape\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mvalidate_shape\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0muse_resource\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0muse_resource\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 425\u001b[0;31m           constraint=constraint)\n\u001b[0m\u001b[1;32m    426\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    427\u001b[0m   def _get_partitioned_variable(\n",
      "\u001b[0;32m/home/hanul/anaconda2/lib/python2.7/site-packages/tensorflow/python/ops/variable_scope.pyc\u001b[0m in \u001b[0;36m_true_getter\u001b[0;34m(name, shape, dtype, initializer, regularizer, reuse, trainable, collections, caching_device, partitioner, validate_shape, use_resource, constraint)\u001b[0m\n\u001b[1;32m    392\u001b[0m           \u001b[0mtrainable\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtrainable\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcollections\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcollections\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    393\u001b[0m           \u001b[0mcaching_device\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcaching_device\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvalidate_shape\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mvalidate_shape\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 394\u001b[0;31m           use_resource=use_resource, constraint=constraint)\n\u001b[0m\u001b[1;32m    395\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    396\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mcustom_getter\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/home/hanul/anaconda2/lib/python2.7/site-packages/tensorflow/python/ops/variable_scope.pyc\u001b[0m in \u001b[0;36m_get_single_variable\u001b[0;34m(self, name, shape, dtype, initializer, regularizer, partition_info, reuse, trainable, collections, caching_device, validate_shape, use_resource, constraint)\u001b[0m\n\u001b[1;32m    740\u001b[0m                          \u001b[0;34m\"reuse=tf.AUTO_REUSE in VarScope? \"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    741\u001b[0m                          \"Originally defined at:\\n\\n%s\" % (\n\u001b[0;32m--> 742\u001b[0;31m                              name, \"\".join(traceback.format_list(tb))))\n\u001b[0m\u001b[1;32m    743\u001b[0m       \u001b[0mfound_var\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_vars\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    744\u001b[0m       \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mshape\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mis_compatible_with\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfound_var\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_shape\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mValueError\u001b[0m: Variable v already exists, disallowed. Did you mean to set reuse=True or reuse=tf.AUTO_REUSE in VarScope? Originally defined at:\n\n  File \"<ipython-input-28-ed56b01b144e>\", line 1, in <module>\n    v = tf.get_variable(\"v\", [1])\n  File \"/home/hanul/anaconda2/lib/python2.7/site-packages/IPython/core/interactiveshell.py\", line 2882, in run_code\n    exec(code_obj, self.user_global_ns, self.user_ns)\n  File \"/home/hanul/anaconda2/lib/python2.7/site-packages/IPython/core/interactiveshell.py\", line 2822, in run_ast_nodes\n    if self.run_code(code, result):\n"
     ]
    }
   ],
   "source": [
    "v = tf.get_variable(\"v\", [1])\n",
    "v1 = tf.get_variable(\"v\", [1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- variable_scope는 scope 내의 정의된 변수 이름에 scope 이름을 prefix로 붙여 구분지어준다.\n",
    "- scope안에 다른 scope을 recursive하게 정의가 가능. 변수명의 prefix는 가장 밖부터 쌓이게 된다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "foo/v:0\n",
      "foo/bar/v:0\n"
     ]
    }
   ],
   "source": [
    "tf.reset_default_graph()\n",
    "\n",
    "with tf.variable_scope(\"foo\"):\n",
    "    v = tf.get_variable(\"v\", [1])\n",
    "print(v.name)\n",
    "\n",
    "tf.reset_default_graph()\n",
    "\n",
    "with tf.variable_scope(\"foo\"):\n",
    "    with tf.variable_scope(\"bar\"):\n",
    "        v = tf.get_variable(\"v\", [1])\n",
    "print(v.name)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- variable scope를 정의하고, 그 안에서 정의된 variable은 다시 정의되었을때 재사용하도록 지정할 수 있다.\n",
    "- variable_scope 내의 option reuse를 True로 두면 scope 내의 변수들은 재사용된다.\n",
    "- tf.get_variable_scope().reuse_variables() 호출 이후 scope 내의 변수들은 재사용된다.\n",
    "- 한 scope내에서 변수들을 재사용하도록 변경 설정하였으면, 다시 재사용 못하도록 물르는 것은 불가능하다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "tf.reset_default_graph()\n",
    "\n",
    "with tf.variable_scope(\"foo\"):\n",
    "    v = tf.get_variable(\"v\", [1])\n",
    "with tf.variable_scope(\"foo\", reuse=True):\n",
    "    v1 = tf.get_variable(\"v\", [1])\n",
    "assert v1 is v     # v1 = v.\n",
    "\n",
    "tf.reset_default_graph()\n",
    "\n",
    "with tf.variable_scope(\"foo\"):                  # tf.get_variable_scope().reuse == False\n",
    "    v = tf.get_variable(\"v\", [1])\n",
    "    tf.get_variable_scope().reuse_variables()   # tf.get_variable_scope().reuse == True\t\n",
    "    v1 = tf.get_variable(\"v\", [1])\n",
    "assert v1 is v"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- scope 내에서 reuse할 변수와 그렇지 않을 변수들을 나눠 scope로 구분할 수 있다.\n",
    "- 변수의 재사용성 여부는 내부 scope에 그대로 상속된다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "with tf.variable_scope(\"root\"):\n",
    "    # At start, the scope is not reusing.\n",
    "    print \"reuse root ?   \", tf.get_variable_scope().reuse\n",
    "    with tf.variable_scope(\"foo\"):\n",
    "        # Opened a sub-scope, still not reusing.\n",
    "        print \"reuse root/foo ?   \", tf.get_variable_scope().reuse\n",
    "    with tf.variable_scope(\"foo\", reuse=True):\n",
    "        # Explicitly opened a reusing scope.\n",
    "        print \"reuse root/foo ?   \", tf.get_variable_scope().reuse\n",
    "        with tf.variable_scope(\"bar\"):\n",
    "            # Now sub-scope inherits the reuse flag.\n",
    "            print \"reuse root/foo/bar ?   \", tf.get_variable_scope().reuse\n",
    "    # Exited the reusing scope, back to a non-reusing one.\n",
    "    print \"reuse root ?   \", tf.get_variable_scope().reuse"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "tf.reset_default_graph()\n",
    "\n",
    "with tf.variable_scope(\"foo\") as foo_scope:\n",
    "    v = tf.get_variable(\"v\", [1])\n",
    "with tf.variable_scope(foo_scope):\n",
    "    w = tf.get_variable(\"w\", [1])\n",
    "with tf.variable_scope(foo_scope, reuse=True):\n",
    "    v1 = tf.get_variable(\"v\", [1])\n",
    "    w1 = tf.get_variable(\"w\", [1])\n",
    "assert v1 is v\n",
    "assert w1 is w"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- 내부 scope에서 외부 scope으로의 접근이 가능하다. \n",
    "- 이때 기존 scope의 위치는 무시하고, 접근한 scope의 위치와 상태를 그대로 받는다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "foo_scope.name : foo\n",
      "other_scope.name : bar/baz\n",
      "foo_scope2.name : foo\n"
     ]
    }
   ],
   "source": [
    "tf.reset_default_graph()\n",
    "\n",
    "with tf.variable_scope(\"foo\") as foo_scope:\n",
    "    print(\"foo_scope.name : %s\" % foo_scope.name)\n",
    "with tf.variable_scope(\"bar\"):\n",
    "    with tf.variable_scope(\"baz\") as other_scope:\n",
    "        print(\"other_scope.name : %s\" % other_scope.name)\n",
    "        with tf.variable_scope(foo_scope) as foo_scope2:\n",
    "            print(\"foo_scope2.name : %s\" % foo_scope2.name)  # Not changed.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- variable scope의 또 다른 기능은, get_variable의 default initializer를 'initializer' 옵션으로 미리 지정해줄 수 있다.\n",
    "- 따로 정의를 해주지 않는 이상, default initializer는 내부 scope들에게도 그대로 상속된다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# clear all created variables, close session and reopen\n",
    "tf.reset_default_graph()\n",
    "sess.close()\n",
    "sess = tf.Session()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "foo_v : 0.4\n",
      "foo_w : 0.3\n",
      "foo_bar_v : 0.4\n",
      "foo_baz_v : 0.1924\n"
     ]
    }
   ],
   "source": [
    "with tf.variable_scope(\"foo\", initializer=tf.constant_initializer(0.4)):\n",
    "    foo_v = tf.get_variable(\"v\", [1])\n",
    "    foo_w = tf.get_variable(\"w\", [1], initializer=tf.constant_initializer(0.3))\n",
    "    with tf.variable_scope(\"bar\"):\n",
    "        foo_bar_v = tf.get_variable(\"v\", [1])    \n",
    "    with tf.variable_scope(\"baz\", initializer=tf.random_normal_initializer()):\n",
    "        foo_baz_v = tf.get_variable(\"v\", [1])\n",
    "        \n",
    "    sess.run(tf.global_variables_initializer())\n",
    "    print(\"foo_v : %.1f\" % foo_v.eval(session=sess)) # Default initializer as set above.\n",
    "    print(\"foo_w : %.1f\" % foo_w.eval(session=sess)) # Specific initializer overrides the default.\n",
    "    print(\"foo_bar_v : %.1f\" % foo_bar_v.eval(session=sess)) # Inherited default initializer.\n",
    "    print(\"foo_baz_v : %.4f\" % foo_baz_v.eval(session=sess))  # Changed default initializer."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.2 Solving the previous problem\n",
    "- get variable로 variable을 정의하고\n",
    "- layer마다 variable scope을 달리하여 weight들을 구분해준다. \n",
    "- layer 생성할 때마다 layer 추가하는 함수를 호출하여, 코드의 재사용성을 높일 수 있다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def conv_relu(input, kernel_shape, bias_shape):\n",
    "    # Create variable named \"weights\", “biases”.\n",
    "    weights = tf.get_variable(\"weights\", kernel_shape, \tinitializer=tf.random_normal_initializer())\n",
    "    biases = tf.get_variable(\"biases\", bias_shape, initializer=tf.constant_initializer(0.0))\n",
    "    conv = tf.nn.conv2d(input, weights, strides=[1, 1, 1, 1], padding='SAME')\n",
    "    return tf.nn.relu(conv + biases)\n",
    "\n",
    "def my_image_filter(input_images):\n",
    "    with tf.variable_scope(\"conv1\"):\n",
    "        # Variables created here will be named \"conv1/weights\", \"conv1/biases\".\n",
    "        relu1 = conv_relu(input_images, [5, 5, 3, 32], [32])\n",
    "    with tf.variable_scope(\"conv2\"):\n",
    "        # Variables created here will be named \"conv2/weights\", \"conv2/biases\".\n",
    "        return conv_relu(relu1, [5, 5, 32, 32], [32])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "with tf.variable_scope(\"image_filters\") as scope:\n",
    "    result1 = my_image_filter(image1)\n",
    "    scope.reuse_variables()\n",
    "    result2 = my_image_filter(image2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.3. Name Scope\n",
    "- name scope은 tf.Variable 로 정의한 variable과 operation의 이름을 관리해준다.\n",
    "- get_variable에 의해 정의한 variable은 무시한다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "foo/add\n"
     ]
    }
   ],
   "source": [
    "tf.reset_default_graph()\n",
    "\n",
    "with tf.variable_scope(\"foo\"):\n",
    "    x = 1.0 + tf.get_variable(\"v\", [1])\n",
    "print(x.op.name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "v name : foo/v:0\n",
      "w name : foo/bar/Variable:0\n",
      "x operation name : foo/bar/add\n"
     ]
    }
   ],
   "source": [
    "tf.reset_default_graph()\n",
    "\n",
    "with tf.variable_scope(\"foo\"):\n",
    "    with tf.name_scope(\"bar\"):\n",
    "        v = tf.get_variable(\"v\", [1])\n",
    "        w = tf.Variable([.1], tf.float32)\n",
    "        x = 1.0 + v\n",
    "print(\"v name : %s\" % v.name)\n",
    "print(\"w name : %s\" % w.name)\n",
    "print(\"x operation name : %s\" % x.op.name)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "tf.constant와 tf.add, tf.multiply, tf.subtract, tf.pow를 이용하여 (1+5)^2 - 5*6 을 수행하는 tensorflow 프로그램을 구현한다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "\n",
    "a = \n",
    "b = \n",
    "a_add_b = \n",
    "pow_two = \n",
    "c = \n",
    "d = \n",
    "c_mul_d = \n",
    "subtraction = \n",
    "\n",
    "# 세션을 생성하고 초기화합니다.\n",
    "sess = \n",
    "\n",
    "# sess.run 을 통해 (1+5)^2 - 5*6 을 계산하는 노드를 수행합니다.\n",
    "print(sess.run(_____))\n",
    "\n",
    "sess.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "tf.variable_scope과 tf.get_variable을 이용하여 \"conv1/weigts, conv1/bias, conv2/weights, conv2/bias\" 변수를 선언한다. conv1/weights과 conv2/weights의 shape은 [3, 3, 1, 10]이고 conv1/bias와 conv2/bias의 shape은 [10]이다. 각 weights 변수들의 initializer는 random_normal_initializer()를 사용하고, bias는 constant_initializer()로 0으로 초기화 한다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "\n",
    "tf.reset_default_graph()\n",
    "\n",
    "with \n",
    "    w1 = \n",
    "    b1 = \n",
    "with \n",
    "    w2 = \n",
    "    b2 = \n",
    "\n",
    "print(w1.name)\n",
    "print(b1.name)\n",
    "print(w2.name)\n",
    "print(b2.name)\n",
    "\n",
    "print(tf.global_variables())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### W = tf.Variable(tf.random_uniform([1], minval=-10, maxval=10), name=\"var\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ 9.06183052]\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "\n",
    "tf.reset_default_graph()\n",
    "\n",
    "W = tf.Variable(tf.random_uniform([1], minval=-10, maxval=10), name=\"var\")\n",
    "\n",
    "# 세션을 생성하고 초기화합니다.\n",
    "sess = tf.Session()\n",
    "\n",
    "# tf.Variable 들을 초기화합니다.\n",
    "init = tf.global_variables_initializer()\n",
    "sess.run(init)\n",
    "\n",
    "print(sess.run(W))\n",
    "\n",
    "sess.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "placeholder() takes at least 1 argument (0 given)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-9-48ce4bf50f37>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     12\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     13\u001b[0m \u001b[0;31m# loss function에 사용될 placeholder를 선언해 줍니다.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 14\u001b[0;31m \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mplaceholder\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     15\u001b[0m \u001b[0my\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mplaceholder\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     16\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mTypeError\u001b[0m: placeholder() takes at least 1 argument (0 given)"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "\n",
    "tf.reset_default_graph()\n",
    "\n",
    "# X 와 Y 의 상관관계를 분석하는 기초적인 선형 회귀 모델을 만들고 실행해봅니다.\n",
    "x_data = [1, 2, 3, 4]\n",
    "y_data = [2, 7, 12, 17]\n",
    "\n",
    "# Weight W와 bias b를 -10 ~ 10 사이의 random 값으로 생성해 줍니다.\n",
    "W = tf.Variable(tf.random_uniform(shape=[1], minval=-10, maxval=10))\n",
    "b = tf.random_uniform(shape=[1], minval=-10, maxval=10)\n",
    "\n",
    "# loss function에 사용될 placeholder를 선언해 줍니다.\n",
    "x = tf.placeholder(tf.float32)\n",
    "y = tf.placeholder(tf.float32)\n",
    "\n",
    "print(x)\n",
    "print(y)\n",
    "\n",
    "# X 와 Y 의 상관 관계를 분석하기 위한 수식을 작성합니다.\n",
    "y_ = W * x + b\n",
    "\n",
    "# loss function을 구현합니다.\n",
    "# loss = mean(y_ - y)^2 : 예측값과 실제값의 거리를 loss로 정합니다.\n",
    "loss = tf.reduce_mean(tf.pow(tf.subtract(y_ - y),2))\n",
    "\n",
    "# 텐서플로우에 기본적으로 포함되어 있는 GradientDesentOptimizer를 이용해 경사 하강법 최적화를 수행합니다.\n",
    "# learning rate은 0.01을 사용합니다.\n",
    "optimizer = tf.train.GradientDescentOptimizer(0.01)\n",
    "\n",
    "# loss을 최소화 하는 것이 최종 목표이므로 minimize를 적용하는 train_op을 구현합니다.\n",
    "train_op = optimizer.minimize(loss)\n",
    "\n",
    "# 세션을 생성하고 초기화합니다.\n",
    "sess = tf.Session()\n",
    "\n",
    "# tf.Variable 들을 초기화합니다.\n",
    "init = tf.global_variable_initializer()\n",
    "sess.run(init)\n",
    "\n",
    "# 최적화를 1000번 수행합니다.\n",
    "for step in range(1000):\n",
    "    # sess.run 을 통해 train_op 와 cost 그래프를 계산합니다.\n",
    "    # 이 때, 가설 수식에 넣어야 할 실제값을 feed_dict 을 통해 전달합니다.\n",
    "    # Todo\n",
    "    _ , l= sess.run([train_op, loss], {x:x_data, y:y_data})\n",
    "    # 매 50회 step마다 중간 결과를 출력합니다.\n",
    "    if (step % 50 == 0):\n",
    "        print(\"loss: \", l)\n",
    "    \n",
    "# 최적화가 완료된 모델에 테스트 값을 넣고 결과가 잘 나오는지 확인해봅니다.\n",
    "print(\"\\n=== Test ===\")\n",
    "print(\"X: 5, Y:\", sess.run(hypothesis, feed_dict={x: 5}))\n",
    "print(\"X: 6, Y:\", sess.run(hypothesis, feed_dict={x: 6}))\n",
    "print(\"X: 7, Y:\", sess.run(hypothesis, feed_dict={x: 7}))\n",
    "print(\"X: 8, Y:\", sess.run(hypothesis, feed_dict={x: 8}))\n",
    "\n",
    "sess.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
